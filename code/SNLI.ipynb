{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ADL2_fast.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Y34guDXt-p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOC04JCOYT6F"
      },
      "source": [
        "from torchtext import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mhv3-euYWUE",
        "outputId": "d620e179-7e7a-4a24-cc63-e4e925d97d29"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenizer(text): # create a tokenizer function\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import copy\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "sentences = data.Field(lower=True, tokenize=tokenizer,include_lengths=True)\n",
        "ans = data.Field(sequential=False)\n",
        "\n",
        "train, dev, test = datasets.SNLI.splits(sentences, ans, root='.dataset')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV8a5Nf-aa-J"
      },
      "source": [
        "sentences.build_vocab(train, dev, test,min_freq=3)\n",
        "ans.build_vocab(train, dev, test)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "Batch_Size=256\n",
        "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
        "            (train, dev, test), batch_size=Batch_Size, device=device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLYKoHtMYY-2"
      },
      "source": [
        "n_layer=1\n",
        "n_layer=1\n",
        "class My_RNN_back(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim,hidden_dim, drop_p):\n",
        "        super(My_RNN_back, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim,\n",
        "                        num_layers=1, dropout=drop_p,bidirectional=True)\n",
        "\n",
        "    def forward(self, inputs, text_lengths):\n",
        "        \n",
        "        #print(inputs.shape)\n",
        "\n",
        "        text_lengths = text_lengths.cpu()\n",
        "\n",
        "        batch_size = inputs.size()[1]\n",
        "\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(inputs, text_lengths, enforce_sorted = False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "\n",
        "        outputs=outputs.view(-1, batch_size, 2, self.hidden_dim)\n",
        "\n",
        "        return outputs[:,:,1,:].view(-1, batch_size, self.hidden_dim)\n",
        "\n",
        "class My_RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim,hidden_dim,drop_p):\n",
        "        super(My_RNN, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim+hidden_dim, hidden_size=hidden_dim,\n",
        "                        num_layers=1, dropout=drop_p,bidirectional=False)#True)\n",
        "\n",
        "    def forward(self, inputs, hidden_backward, text_lengths):\n",
        "        \n",
        "        text_lengths = text_lengths.cpu()\n",
        "        \n",
        "        batch_size = inputs.size()[1]\n",
        "        inputs=torch.cat([inputs,hidden_backward],2)\n",
        "        #print(inputs.shape)\n",
        "\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(inputs, text_lengths, enforce_sorted = False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "\n",
        "        return outputs #ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)  \n",
        "\n",
        "class My_RNN1(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim,hidden_dim,drop_p):\n",
        "        super(My_RNN1, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.LSTM(input_size=embed_dim+hidden_dim, hidden_size=hidden_dim,\n",
        "                        num_layers=1, dropout=drop_p,bidirectional=False)#True)\n",
        "\n",
        "    def forward(self, inputs, hidden_backward, text_lengths):\n",
        "\n",
        "         \n",
        "        text_lengths = text_lengths.cpu()\n",
        "\n",
        "        batch_size = inputs.size()[1]\n",
        "\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(inputs, text_lengths, enforce_sorted = False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        #print(outputs.shape)\n",
        "\n",
        "        #outputs=outputs.view(-1,batch_size,2,self.hidden_dim)\n",
        "\n",
        "\n",
        "        return outputs[-1,:,:].view(batch_size, -1) #ht[-2:].transpose(0, 1).contiguous().view(batch_size, -1)    \n",
        "\n",
        "\n",
        "class My_Bi_LSTM(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, drop_p, n_layers=1):\n",
        "        super(My_Bi_LSTM, self).__init__()\n",
        "        self.rnn_back=[]\n",
        "        self.rnn=[]\n",
        "        self.n_layers=n_layers\n",
        "        for i in range(int(n_layers)):\n",
        "            if (i==0):\n",
        "                if (i==(n_layers-1)):\n",
        "                    self.rnn_back.append(My_RNN_back(embed_dim,hidden_dim, drop_p).to(device))\n",
        "                    self.rnn.append(My_RNN1(embed_dim,hidden_dim, drop_p).to(device))\n",
        "                else:    \n",
        "                    self.rnn_back.append(My_RNN_back(embed_dim,hidden_dim, drop_p).to(device))\n",
        "                    self.rnn.append(My_RNN(embed_dim,hidden_dim, drop_p).to(device))\n",
        "            else:\n",
        "                \n",
        "                if (i==(n_layers-1)):\n",
        "                    self.rnn_back.append(My_RNN_back(2*hidden_dim,hidden_dim, drop_p).to(device))\n",
        "                    self.rnn.append(My_RNN1(hidden_dim,hidden_dim, drop_p).to(device))\n",
        "                else:    \n",
        "                    self.rnn_back.append(My_RNN_back(2*hidden_dim,hidden_dim, drop_p).to(device))\n",
        "                    self.rnn.append(My_RNN(hidden_dim,hidden_dim, drop_p).to(device))\n",
        "        self.rnn_back = nn.ModuleList(self.rnn_back)\n",
        "        self.rnn = nn.ModuleList(self.rnn)\n",
        "\n",
        "    def forward(self, embeddings, embeddings_len):\n",
        "        \n",
        "        temp_output=embeddings\n",
        "        \n",
        "        batch_size = embeddings.size()[1]\n",
        "        \n",
        "        for i in range(self.n_layers):\n",
        "            \n",
        "            if (i==(self.n_layers-1)):\n",
        "                hid_back=self.rnn_back[i](temp_output, embeddings_len)\n",
        "                out1 = self.rnn[i](temp_output,hid_back, embeddings_len)\n",
        "                temp_output=torch.cat([hid_back[0,:,:].view(batch_size, -1), out1], 1)\n",
        "\n",
        "\n",
        "            else:\n",
        "                if (i==0):\n",
        "                    hid_back=self.rnn_back[i](temp_output, embeddings_len)\n",
        "                    out1 = self.rnn[i](temp_output, hid_back, embeddings_len)\n",
        "                    temp_output=torch.cat([hid_back, out1], 2)\n",
        "                \n",
        "                else:\n",
        "                    hid_back=self.rnn_back[i](temp_output, embeddings_len)\n",
        "                    out1 = self.rnn[i](temp_output, hid_back, embeddings_len)\n",
        "                    temp_output=torch.cat([hid_back, out1], 2)\n",
        "        return temp_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Output(nn.Module):\n",
        "\n",
        "    def __init__(self, out_dim,inp_dim,drop_p):\n",
        "        super(Output, self).__init__()\n",
        "        self.fc1=nn.Linear(inp_dim,int(inp_dim/2))\n",
        "        #self.fc2=nn.Linear(int(inp_dim/2),int(inp_dim/2))\n",
        "        self.fc3=nn.Linear(int(inp_dim/2),int(inp_dim/4))\n",
        "        self.fc4=nn.Linear(int(inp_dim/4),out_dim)\n",
        "        self.p=drop_p\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=F.dropout(F.relu(self.fc1(x)),p=self.p)\n",
        "        #x=F.dropout(F.relu(self.fc2(x)),p=self.p)\n",
        "        x=F.dropout(F.relu(self.fc3(x)),p=self.p)\n",
        "        x=(self.fc4(x))\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CfWFfcWwJbu"
      },
      "source": [
        "hidden_dim=128\n",
        "embed_dim=200\n",
        "out_dim=4\n",
        "drop_p1=0.25\n",
        "drop_p2=0.4\n",
        "class Classifier2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier,self).__init__()\n",
        "        self.embedding=nn.Embedding(len(sentences.vocab),embed_dim)\n",
        "        self.RNN_add=My_RNN_back(embed_dim,hidden_dim,drop_p1)\n",
        "        self.RNN=My_RNN(embed_dim,hidden_dim,drop_p1)\n",
        "        #self.final_l=Output(out_dim,4*hidden_dim,drop_p2)\n",
        "        self.final_l=Output(out_dim,2*hidden_dim,drop_p2)\n",
        "        \n",
        "    def forward(self,batch):\n",
        "        sen1 = self.embedding(batch.premise)\n",
        "        sen2 = self.embedding(batch.hypothesis)\n",
        "        hid_back=self.RNN_add(sen1)\n",
        "        hid_back2=self.RNN_add(sen2)\n",
        "        premise = self.RNN(sen1,hid_back)\n",
        "        hypothesis = self.RNN(sen2,hid_back2)\n",
        "        out = self.final_l(torch.cat([premise, hypothesis], 1))\n",
        "        return out\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self,n_layer):\n",
        "        super(Classifier,self).__init__()\n",
        "        self.embedding=nn.Embedding(len(sentences.vocab),embed_dim)\n",
        "        self.RNN=My_Bi_LSTM(embed_dim,hidden_dim,drop_p1,n_layer)\n",
        "        self.final_l=Output(out_dim,4*hidden_dim,drop_p2)\n",
        "        \n",
        "    def forward(self,batch):\n",
        "        prem,prem_len=batch.premise\n",
        "        hypo,hypo_len=batch.hypothesis\n",
        "        sen1 = self.embedding(prem)\n",
        "        sen2 = self.embedding(hypo)\n",
        "        premise = self.RNN(sen1,prem_len)\n",
        "        hypothesis = self.RNN(sen2,hypo_len)\n",
        "        out = self.final_l(torch.cat([premise, hypothesis], 1))\n",
        "        return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiDPgXcOY7_Q"
      },
      "source": [
        "def train(model,train_loader,val_loader,optimizer,criterion,scheduler,epochs,print_iter=5):\n",
        "    train_loss=[]\n",
        "    val_loss=[]\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        train_loader.init_epoch()\n",
        "        running_loss_train=0 \n",
        "        total=0.0\n",
        "        for indx,inputs in enumerate(train_loader):\n",
        "            #inputs=inputs.to(device)\n",
        "            #labels=labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output=model(inputs)\n",
        "            loss=criterion(output,inputs.label)\n",
        "            running_loss_train+=loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total+=inputs.batch_size\n",
        "        train_loss.append(running_loss_train/total)\n",
        "        if (i%print_iter)==0:\n",
        "            model.eval()\n",
        "            running_corrects=0.0\n",
        "            running_loss=0.0\n",
        "            total=0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs in val_loader:\n",
        "                    #inputs=inputs.to(device)\n",
        "                    #labels=labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    output=model(inputs)\n",
        "                    loss=criterion(output,inputs.label)\n",
        "                    _,pred=torch.max(output, 1)\n",
        "                    running_corrects += torch.sum(pred == inputs.label).item()\n",
        "                    running_loss+=loss.item()\n",
        "                    total+=inputs.batch_size\n",
        "            print(' {} Loss: {:.6f} Acc: {:.6f}'.format(\n",
        "                  i,running_loss/total,(running_corrects/total)))\n",
        "            val_loss.append(running_loss/total)\n",
        "        scheduler.step()\n",
        "    return model,train_loss,val_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3-uKGRyQf7H",
        "outputId": "08f117be-0073-41c2-d669-9c9f6cb4db96"
      },
      "source": [
        "model2=Classifier(2)\n",
        "model2.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (embedding): Embedding(21571, 200)\n",
              "  (RNN): My_Bi_LSTM(\n",
              "    (rnn_back): ModuleList(\n",
              "      (0): My_RNN_back(\n",
              "        (rnn): LSTM(200, 128, dropout=0.25, bidirectional=True)\n",
              "      )\n",
              "      (1): My_RNN_back(\n",
              "        (rnn): LSTM(256, 128, dropout=0.25, bidirectional=True)\n",
              "      )\n",
              "    )\n",
              "    (rnn): ModuleList(\n",
              "      (0): My_RNN(\n",
              "        (rnn): LSTM(328, 128, dropout=0.25)\n",
              "      )\n",
              "      (1): My_RNN1(\n",
              "        (rnn): LSTM(256, 128, dropout=0.25)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_l): Output(\n",
              "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (fc4): Linear(in_features=128, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owESAG1pZALx",
        "outputId": "b3e273f7-645f-4cee-fa90-f69af6db726d"
      },
      "source": [
        "import torch.optim as optim\n",
        "lr=0.001\n",
        "optimizer2=optim.Adam(model2.parameters(),lr,weight_decay=0.0001)\n",
        "criterion2=nn.CrossEntropyLoss()\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "exp_lr_scheduler2 = optim.lr_scheduler.StepLR(optimizer2, step_size=4, gamma=0.5)\n",
        "model2.to(device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (embedding): Embedding(21571, 200)\n",
              "  (RNN): My_Bi_LSTM(\n",
              "    (rnn_back): ModuleList(\n",
              "      (0): My_RNN_back(\n",
              "        (rnn): LSTM(200, 128, dropout=0.25, bidirectional=True)\n",
              "      )\n",
              "      (1): My_RNN_back(\n",
              "        (rnn): LSTM(256, 128, dropout=0.25, bidirectional=True)\n",
              "      )\n",
              "    )\n",
              "    (rnn): ModuleList(\n",
              "      (0): My_RNN(\n",
              "        (rnn): LSTM(328, 128, dropout=0.25)\n",
              "      )\n",
              "      (1): My_RNN1(\n",
              "        (rnn): LSTM(256, 128, dropout=0.25)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_l): Output(\n",
              "    (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (fc4): Linear(in_features=128, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "qKEuT4IQZELP",
        "outputId": "74f8defa-c349-4d4b-a190-c556ae641162"
      },
      "source": [
        "_,train_loss,val_loss=train(model2,train_iter,dev_iter,optimizer2,criterion2,exp_lr_scheduler2,epochs=20,print_iter=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0 Loss: 0.002955 Acc: 0.680045\n",
            " 1 Loss: 0.002694 Acc: 0.720382\n",
            " 2 Loss: 0.002596 Acc: 0.731762\n",
            " 3 Loss: 0.002500 Acc: 0.741821\n",
            " 4 Loss: 0.002390 Acc: 0.755029\n",
            " 5 Loss: 0.002348 Acc: 0.764174\n",
            " 6 Loss: 0.002337 Acc: 0.768746\n",
            " 7 Loss: 0.002342 Acc: 0.764784\n",
            " 8 Loss: 0.002272 Acc: 0.772201\n",
            " 9 Loss: 0.002252 Acc: 0.777383\n",
            " 10 Loss: 0.002244 Acc: 0.778602\n",
            " 11 Loss: 0.002249 Acc: 0.780431\n",
            " 12 Loss: 0.002252 Acc: 0.777078\n",
            " 13 Loss: 0.002251 Acc: 0.777992\n",
            " 14 Loss: 0.002233 Acc: 0.782666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-98693ec00a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_lr_scheduler2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-1b8f46172397>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, print_iter)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mrunning_loss_train\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G0b0iG0a0C-"
      },
      "source": [
        "def accuracy(model,train_loader):\n",
        "    model.eval()\n",
        "    running_corrects=0.0\n",
        "    running_loss=0.0\n",
        "    total=0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs in train_loader:\n",
        "            #inputs=inputs.to(device)\n",
        "            #labels=labels.to(device)\n",
        "            output=model(inputs)\n",
        "            _,pred=torch.max(output, 1)\n",
        "            running_corrects += torch.sum(pred == inputs.label)\n",
        "            total+=inputs.batch_size\n",
        "    print(' Acc: {:.6f}'.format((running_corrects/total)))\n",
        "    return running_corrects/total"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSCp4yFT1fLk",
        "outputId": "255c4ad2-362b-4017-879d-022139be6861"
      },
      "source": [
        "accuracy(model2,test_iter)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Acc: 0.774430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7744, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICSQXN-r1mjB"
      },
      "source": [
        "torch.save(model2.state_dict(), 'hs250.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-nuYhH1y6H"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.ion()\n",
        "plt.figure()\n",
        "plt.plot(train_loss,label='train_loss')\n",
        "plt.plot(val_loss,label='validation_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.savefig(\"./loss1.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h5DPGM8MjRD_"
      },
      "source": [
        "plt.savefig(\"./loss.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pOefK0dmjfXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WcHUsgWmRa7d"
      },
      "source": [
        "torch.save(sentences,'models/vocab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XPx1YhaNRa7f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}